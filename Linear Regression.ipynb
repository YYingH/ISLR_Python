{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det\n",
    "from numpy.linalg import inv\n",
    "from numpy import mat\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Regression.csv')\n",
    "x_train = data['x'].values\n",
    "y_train = data['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "Loss function：\n",
    "$$L(w) = \\frac{1}{2M}\\sum_{i=1}^{m}(y-x_iw)^2$$\n",
    "\n",
    "闭解式：$W=(X^TX)^{-1}X^TY$\n",
    "    \n",
    "如果 $X^TX$ 没有逆矩阵， 则不能使用这种方法，可以采用梯度下降等优化方法求近似解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression():\n",
    "    x_mat = mat(x_train).T\n",
    "    y_mat = mat(y_train).T\n",
    "    [m,n] = x_mat.shape\n",
    "    x_mat = np.hstack((x_mat, mat(np.ones((m,1)))))\n",
    "    weight = mat(random.rand(n+1, 1))\n",
    "    if det(x_mat.T*x_mat) == 0:\n",
    "        print('the det of xTx is equal to zero')\n",
    "        return\n",
    "    else:\n",
    "        weight = inv(x_mat.T*x_mat) * x_mat.T*y_mat\n",
    "    \n",
    "    x_min = x_train.min()\n",
    "    x_max = x_train.max()\n",
    "    y_min = weight[0]*x_min + weight[1]\n",
    "    y_max = weight[0]*x_max + weight[1]\n",
    "    plt.scatter(x_train, y_train)\n",
    "    plt.plot([x_min, x_max],[y_min[0,0],y_max[0,0]],'-g')\n",
    "    plt.show()\n",
    "    \n",
    "linearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge regression\n",
    "\n",
    "add regularization for linear regression\n",
    "\n",
    "Loss function:\n",
    "$$L(w) = \\frac{1}{2m}\\sum_{i=1}^{m}(y-x_iw)^2 + \\lambda\\sum_{i=1}^{n}w_{i}^2$$\n",
    "\n",
    "闭解式：$W = (X^TX + \\lambda{I})^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeRegression(x_train, y_train, lam = 0.2):\n",
    "    x_mat = mat(x_train).T\n",
    "    [m,n] = np.shape(x_mat)\n",
    "    x_mat = np.hstack((x_mat, mat(np.ones((m,1)))))\n",
    "    y_mat = mat(y_train).T\n",
    "    \n",
    "    weight = mat(random.rand(n+1,1))\n",
    "    xTx = x_mat.T * x_mat + lam * mat(np.eye(n))\n",
    "    \n",
    "    if det(xTx) == 0:\n",
    "        print('the det of xTx is zero')\n",
    "        return\n",
    "    weight = xTx.I * x_mat.T * y_mat\n",
    "    \n",
    "    x_min = x_train.min()\n",
    "    x_max = x_train.max()\n",
    "    y_min = weight[0]*x_min + weight[1]\n",
    "    y_max = weight[0]*x_max + weight[1]\n",
    "    plt.scatter(x_train, y_train)\n",
    "    plt.plot([x_min, x_max],[y_min[0,0],y_max[0,0]],'-g')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "ridgeRegression(x_train, y_train, lam = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso regression\n",
    "\n",
    "add l1 regularization\n",
    "\n",
    "Loss function:\n",
    "\n",
    "$$L(w) = \\frac{1}{2m}\\sum_{i=1}^{m}(y-x_iw)^2 + \\lambda \\sum_{i=1}^{n}\\left | w_i \\right |$$\n",
    "\n",
    "这里不能采用闭解式， 可以采用向前逐步回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(x_train, y_train, eps=0.001, itr_num=1000):\n",
    "    x_mat = mat(x_train).T\n",
    "    [m,n] = np.shape(x_mat)\n",
    "    x_mat = (x_mat - x_mat.mean(axis = 0))/x_mat.std(axis=0)\n",
    "    x_mat = np.hstack((x_mat, mat(np.ones((m, 1)))))\n",
    "    y_mat = mat(y_train).T\n",
    "    y_mat = (y_mat - y_mat.mean(axis = 0))/y_mat.std(axis=0)\n",
    "    weight = mat(random.rand(n+1, 1))\n",
    "    best_weight = weight.copy()\n",
    "    \n",
    "    for i in range(itr_num):\n",
    "#         print(weight.T)\n",
    "        lowest_error = np.inf\n",
    "        for j in range(n+1):\n",
    "            for sign in [-1,1]:\n",
    "                weight_copy = weight.copy()\n",
    "                weight_copy[j] += eps*sign\n",
    "                y_predict = x_mat * weight_copy\n",
    "                error = np.power(y_mat - y_predict,2).sum()\n",
    "                if error < lowest_error:\n",
    "                    lowest_error = error\n",
    "                    best_weight = weight_copy\n",
    "    \n",
    "    weight = best_weight\n",
    "    \n",
    "    x_min = x_train.min()\n",
    "    x_max = x_train.max()\n",
    "    y_min = weight[0]*x_min + weight[1]\n",
    "    y_max = weight[0]*x_max + weight[1]\n",
    "    plt.scatter(x_train, y_train)\n",
    "    plt.plot([x_min, x_max],[y_min[0,0],y_max[0,0]],'-g')\n",
    "    plt.show()\n",
    "\n",
    "lasso_regression(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 局部加权线性回归\n",
    "\n",
    "给待测点附近等每个点赋予一定的权重。\n",
    "\n",
    "损失函数：\n",
    "\n",
    "$$L(\\theta ) = \\frac{1}{2M}\\sum_{i=1}^{m}w_i(y-x_i\\theta)^2$$\n",
    "\n",
    "其中， $w_i$表示第i个样本的权重。\n",
    "\n",
    "局部加权线性回归使用“核”来对附近对点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下：\n",
    "\n",
    "$$exp(\\frac{\\left | w_i-x \\right |}{-2k^2})$$\n",
    "\n",
    "这样就有一个只含对角元素的权重矩阵W，并且点$x_i$与x越近，$w_i$也会越大。这里的参数k决定了对附近的点赋予多大的权重，这也是唯一需要考虑的参数。\n",
    "\n",
    "当k越大，有越多的点被用于训练回归模型；\n",
    "当k越小，有越少的点用于训练回归模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_linear_regression(test_point, x_train, y_train, k=1.0):\n",
    "    x_mat = mat(x_train).T\n",
    "    [m,n] = x_mat.shape\n",
    "    x_mat = np.hstack((x_mat, mat(np.ones((m, 1)))))\n",
    "    y_mat = mat(y_train).T\n",
    "    \n",
    "    weight = mat(np.zeros((n+1, 1)))\n",
    "    y_predict = mat(np.zeros((len(test_point), 1)))\n",
    "    \n",
    "    for i in range(len(test_point)):\n",
    "        test_point_mat = mat(test_point[i])\n",
    "        test_point_mat = np.hstack((test_point_mat, mat([[1]])))\n",
    "        weights = mat(np.eye((m)))\n",
    "        test_data = np.tile(test_point_mat, [m,1])\n",
    "        distances = (test_data - x_mat) * (test_data - x_mat).T / (n+1)\n",
    "        distances = np.exp(distances/ (-2* k **2))\n",
    "        weights = np.diag(np.diag(distances))\n",
    "        # weights = distances * weights\n",
    "        xTx = x_mat.T * (weights * x_mat)\n",
    "        if det(xTx) == 0:\n",
    "            print('the det of xTx is equal to zero')\n",
    "            return\n",
    "        weight = xTx.I * x_mat.T * weights * y_mat\n",
    "        y_predict[i] = test_point_mat * weight\n",
    "    print(y_predict)\n",
    "\n",
    "locally_weighted_linear_regression([[15],[20]], x_train, y_train,k=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISLR_Python",
   "language": "python",
   "name": "islr_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
